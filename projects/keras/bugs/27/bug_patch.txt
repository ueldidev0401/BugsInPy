diff --git a/keras/backend/cntk_backend.py b/keras/backend/cntk_backend.py
index 989a01c6..0d87907c 100644
--- a/keras/backend/cntk_backend.py
+++ b/keras/backend/cntk_backend.py
@@ -1490,6 +1490,12 @@ def rnn(step_function, inputs, initial_states,
         go_backwards=False, mask=None, constants=None,
         unroll=False, input_length=None):
 
+    if not unroll and mask is not None:
+        warnings.warn(
+            'CNTK Backend only supports accurate masking if '
+            '`output == new_states[0]` for '
+            '`output, new_states = step_function(inputs, states)`')
+
     shape = int_shape(inputs)
     dims = len(shape)
 
diff --git a/keras/backend/tensorflow_backend.py b/keras/backend/tensorflow_backend.py
index 8a0806c1..393e6d39 100644
--- a/keras/backend/tensorflow_backend.py
+++ b/keras/backend/tensorflow_backend.py
@@ -2930,7 +2930,7 @@ def rnn(step_function, inputs, initial_states,
         ValueError: If `mask` is provided (not `None`)
             but states is not provided (`len(states)` == 0).
     """
-    ndim = len(inputs.get_shape())
+    ndim = len(inputs.shape)
     if ndim < 3:
         raise ValueError('Input should be at least 3D.')
 
@@ -2942,9 +2942,22 @@ def rnn(step_function, inputs, initial_states,
     if mask is not None:
         if mask.dtype != tf.bool:
             mask = tf.cast(mask, tf.bool)
-        if len(mask.get_shape()) == ndim - 1:
-            mask = expand_dims(mask)
-        mask = tf.transpose(mask, axes)
+        if len(mask.shape) != 2:
+            raise ValueError(
+                'mask should have `shape=(samples, time)`, '
+                'got {}'.format(mask.shape))
+        mask = tf.transpose(mask, [1, 0])
+
+        def get_matching_mask(mask_t, ref_tensor_t):
+            # tf.where needs its condition tensor
+            # to be the same shape as its two
+            # result tensors
+            ndim = len(ref_tensor_t.shape)
+            for _ in range(ndim - 1):
+                mask_t = expand_dims(mask_t)
+            add_shape = tf.shape(ref_tensor_t)[1:]
+            multiple = tf.concat([[1], add_shape], 0)
+            return tf.tile(mask_t, multiple)
 
     if constants is None:
         constants = []
@@ -2953,7 +2966,7 @@ def rnn(step_function, inputs, initial_states,
     uses_learning_phase = False
 
     if unroll:
-        if not inputs.get_shape()[0]:
+        if not inputs.shape[0]:
             raise ValueError('Unrolling requires a '
                              'fixed number of timesteps.')
         states = initial_states
@@ -2974,32 +2987,18 @@ def rnn(step_function, inputs, initial_states,
                 if getattr(output, '_uses_learning_phase', False):
                     uses_learning_phase = True
 
-                # tf.where needs its condition tensor
-                # to be the same shape as its two
-                # result tensors, but in our case
-                # the condition (mask) tensor is
-                # (nsamples, 1), and A and B are (nsamples, ndimensions).
-                # So we need to
-                # broadcast the mask to match the shape of A and B.
-                # That's what the tile call does,
-                # it just repeats the mask along its second dimension
-                # n times.
-                tiled_mask_t = tf.tile(mask_t,
-                                       tf.stack([1, tf.shape(output)[1]]))
-
                 if not successive_outputs:
                     prev_output = zeros_like(output)
                 else:
                     prev_output = successive_outputs[-1]
 
-                output = tf.where(tiled_mask_t, output, prev_output)
+                output_mask_t = get_matching_mask(mask_t, output)
+                output = tf.where(output_mask_t, output, prev_output)
 
                 return_states = []
                 for state, new_state in zip(states, new_states):
-                    # (see earlier comment for tile explanation)
-                    tiled_mask_t = tf.tile(mask_t,
-                                           tf.stack([1, tf.shape(new_state)[1]]))
-                    return_states.append(tf.where(tiled_mask_t,
+                    state_mask_t = get_matching_mask(mask_t, new_state)
+                    return_states.append(tf.where(state_mask_t,
                                                   new_state,
                                                   state))
                 states = return_states
@@ -3026,26 +3025,29 @@ def rnn(step_function, inputs, initial_states,
         states = tuple(initial_states)
 
         time_steps = tf.shape(inputs)[0]
-        outputs, _ = step_function(inputs[0], initial_states + constants)
+        output, _ = step_function(inputs[0], initial_states + constants)
         output_ta = tensor_array_ops.TensorArray(
-            dtype=outputs.dtype,
+            dtype=output.dtype,
             size=time_steps,
             tensor_array_name='output_ta')
+        initial_output = zeros_like(output)
         input_ta = tensor_array_ops.TensorArray(
             dtype=inputs.dtype,
             size=time_steps,
             tensor_array_name='input_ta')
         input_ta = input_ta.unstack(inputs)
         time = tf.constant(0, dtype='int32', name='time')
+        while_loop_kwargs = {
+            'cond': lambda time, *_: time < time_steps,
+            'parallel_iterations': 32,
+            'swap_memory': True,
+            'maximum_iterations': input_length}
 
         if mask is not None:
             if not states:
                 raise ValueError('No initial states provided! '
                                  'When using masking in an RNN, you should '
-                                 'provide initial states '
-                                 '(and your step function should return '
-                                 'as its first state at time `t` '
-                                 'the output at time `t-1`).')
+                                 'provide initial states')
             if go_backwards:
                 mask = reverse(mask, 0)
 
@@ -3055,12 +3057,13 @@ def rnn(step_function, inputs, initial_states,
                 tensor_array_name='mask_ta')
             mask_ta = mask_ta.unstack(mask)
 
-            def _step(time, output_ta_t, *states):
+            def _step(time, output_ta_t, output_tm1, *states):
                 """RNN step function.
 
                 # Arguments
                     time: Current timestep value.
                     output_ta_t: TensorArray.
+                    output_tm1: output Tensor from previous timestep
                     *states: List of states.
 
                 # Returns
@@ -3075,18 +3078,23 @@ def rnn(step_function, inputs, initial_states,
                     global uses_learning_phase
                     uses_learning_phase = True
                 for state, new_state in zip(states, new_states):
-                    new_state.set_shape(state.get_shape())
-                tiled_mask_t = tf.tile(mask_t,
-                                       tf.stack([1, tf.shape(output)[1]]))
-                output = tf.where(tiled_mask_t, output, states[0])
-                tmp = []
-                for i in range(len(states)):
-                    multiples = tf.stack([1, tf.shape(new_states[i])[1]])
-                    tiled = tf.tile(mask_t, multiples)
-                    tmp.append(tf.where(tiled, new_states[i], states[i]))
-                new_states = tmp
+                    new_state.set_shape(state.shape)
+
+                output_mask_t = get_matching_mask(mask_t, output)
+                output = tf.where(output_mask_t, output, output_tm1)
+
+                new_states = [tf.where(get_matching_mask(mask_t, new_states[i]),
+                                       new_states[i],
+                                       states[i]) for i in range(len(states))]
+
                 output_ta_t = output_ta_t.write(time, output)
-                return (time + 1, output_ta_t) + tuple(new_states)
+                return (time + 1, output_ta_t, output) + tuple(new_states)
+
+            final_outputs = control_flow_ops.while_loop(
+                body=_step,
+                loop_vars=(time, output_ta, initial_output) + states,
+                **while_loop_kwargs)
+            new_states = final_outputs[3:]  # skip output_tm1
         else:
             def _step(time, output_ta_t, *states):
                 """RNN step function.
@@ -3107,25 +3115,22 @@ def rnn(step_function, inputs, initial_states,
                     global uses_learning_phase
                     uses_learning_phase = True
                 for state, new_state in zip(states, new_states):
-                    new_state.set_shape(state.get_shape())
+                    new_state.set_shape(state.shape)
                 output_ta_t = output_ta_t.write(time, output)
                 return (time + 1, output_ta_t) + tuple(new_states)
 
-        final_outputs = control_flow_ops.while_loop(
-            cond=lambda time, *_: time < time_steps,
-            body=_step,
-            loop_vars=(time, output_ta) + states,
-            parallel_iterations=32,
-            swap_memory=True,
-            maximum_iterations=input_length)
+            final_outputs = control_flow_ops.while_loop(
+                body=_step,
+                loop_vars=(time, output_ta) + states,
+                **while_loop_kwargs)
+            new_states = final_outputs[2:]
+
         last_time = final_outputs[0]
         output_ta = final_outputs[1]
-        new_states = final_outputs[2:]
-
         outputs = output_ta.stack()
         last_output = output_ta.read(last_time - 1)
 
-    axes = [1, 0] + list(range(2, len(outputs.get_shape())))
+    axes = [1, 0] + list(range(2, len(outputs.shape)))
     outputs = tf.transpose(outputs, axes)
     last_output._uses_learning_phase = uses_learning_phase
     return last_output, outputs, new_states
diff --git a/keras/backend/theano_backend.py b/keras/backend/theano_backend.py
index 868a8b2f..26a3ce1b 100644
--- a/keras/backend/theano_backend.py
+++ b/keras/backend/theano_backend.py
@@ -1503,10 +1503,22 @@ def rnn(step_function, inputs, initial_states,
     uses_learning_phase = False
 
     if mask is not None:
-        if mask.ndim == ndim - 1:
-            mask = expand_dims(mask)
-        assert mask.ndim == ndim
-        mask = mask.dimshuffle(axes)
+        if mask.ndim != 2:
+            raise ValueError(
+                'mask should have `shape=(samples, time)`, '
+                'got {}'.format(mask.shape))
+        mask = mask.dimshuffle([1, 0])
+
+        def get_matching_mask(mask_t, ref_tensor_t):
+            # tf.where needs its condition tensor
+            # to be the same shape as its two
+            # result tensors
+            ndim = ref_tensor_t.ndim
+            for _ in range(ndim - 1):
+                mask_t = expand_dims(mask_t)
+            add_shape = ref_tensor_t.shape[1:]
+            reps = T.concatenate([[1], add_shape], 0)
+            return T.tile(mask_t, reps, ndim=ndim)
 
         if unroll:
             indices = list(range(input_length))
@@ -1526,10 +1538,12 @@ def rnn(step_function, inputs, initial_states,
                 else:
                     prev_output = successive_outputs[-1]
 
-                output = T.switch(mask[i], output, prev_output)
+                output_mask = get_matching_mask(mask[i], output)
+                output = T.switch(output_mask, output, prev_output)
                 kept_states = []
                 for state, new_state in zip(states, new_states):
-                    kept_states.append(T.switch(mask[i], new_state, state))
+                    state_mask = get_matching_mask(mask[i], state)
+                    kept_states.append(T.switch(state_mask, new_state, state))
                 states = kept_states
 
                 successive_outputs.append(output)
@@ -1557,10 +1571,12 @@ def rnn(step_function, inputs, initial_states,
                     global uses_learning_phase
                     uses_learning_phase = True
                 # output previous output if masked.
-                outputs = T.switch(mask, outputs, output_tm1)
+                output_mask = get_matching_mask(mask, outputs)
+                outputs = T.switch(output_mask, outputs, output_tm1)
                 return_states = []
                 for state, new_state in zip(states, new_states):
-                    return_states.append(T.switch(mask, new_state, state))
+                    state_mask = get_matching_mask(mask, state)
+                    return_states.append(T.switch(state_mask, new_state, state))
                 return [outputs] + return_states
 
             results, _ = theano.scan(
diff --git a/tests/keras/backend/backend_test.py b/tests/keras/backend/backend_test.py
index 806edc69..f7f0b5bc 100644
--- a/tests/keras/backend/backend_test.py
+++ b/tests/keras/backend/backend_test.py
@@ -826,6 +826,119 @@ class TestBackend(object):
         assert_allclose(last_y1, last_y2, atol=1e-05)
         assert_allclose(y1, y2, atol=1e-05)
 
+    def test_rnn_output_and_state_masking_independent(self):
+        num_samples = 2
+        num_timesteps = 4
+        state_and_io_size = 5
+        mask_last_num_timesteps = 2  # for second sample only
+
+        # a step function that just outputs inputs,
+        # but increments states +1 per timestep
+        def step_function(inputs, states):
+            return inputs, [s + 1 for s in states]
+
+        inputs_vals = np.random.random(
+            (num_samples, num_timesteps, state_and_io_size))
+        initial_state_vals = np.random.random((num_samples, state_and_io_size))
+        # masking of two last timesteps for second sample only
+        mask_vals = np.ones((num_samples, num_timesteps))
+        mask_vals[1, -mask_last_num_timesteps:] = 0
+
+        # outputs expected to be same as inputs for the first sample
+        expected_outputs = inputs_vals.copy()
+        # but for the second sample all outputs in masked region should be the same
+        # as last output before masked region
+        expected_outputs[1, -mask_last_num_timesteps:] = \
+            expected_outputs[1, -(mask_last_num_timesteps + 1)]
+
+        expected_state = initial_state_vals.copy()
+        # first state should be incremented for every timestep (no masking)
+        expected_state[0] += num_timesteps
+        # second state should not be incremented for last two timesteps
+        expected_state[1] += (num_timesteps - mask_last_num_timesteps)
+
+        # verify same expected output for `unroll=true/false`
+        inputs = K.variable(inputs_vals)
+        initial_states = [K.variable(initial_state_vals)]
+        mask = K.variable(mask_vals)
+        for unroll in [True, False]:
+            last_output, outputs, last_states = K.rnn(
+                step_function,
+                inputs,
+                initial_states,
+                mask=mask,
+                unroll=unroll,
+                input_length=num_timesteps if unroll else None)
+
+            assert_allclose(K.eval(outputs), expected_outputs)
+            assert_allclose(K.eval(last_states[0]), expected_state)
+
+    @pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported')
+    def test_rnn_output_num_dim_larger_than_2_masking(self):
+        num_samples = 3
+        num_timesteps = 4
+        num_features = 5
+
+        def step_function(inputs, states):
+            outputs = K.tile(K.expand_dims(inputs), [1, 1, 2])
+            return outputs, states
+
+        inputs_vals = np.random.random((num_samples, num_timesteps, num_features))
+        initial_state_vals = np.random.random((num_samples, 6))
+        mask_vals = np.ones((num_samples, num_timesteps))
+        mask_vals[-1, -1] = 0  # final timestep masked for last sample
+
+        expected_outputs = np.repeat(inputs_vals[..., None], repeats=2, axis=-1)
+        # for the last sample, the final timestep (in masked region) should be the
+        # same as the second to final output (before masked region)
+        expected_outputs[-1, -1] = expected_outputs[-1, -2]
+
+        inputs = K.variable(inputs_vals)
+        initial_states = [K.variable(initial_state_vals)]
+        mask = K.variable(mask_vals)
+        for unroll in [True, False]:
+            last_output, outputs, last_states = K.rnn(
+                step_function,
+                inputs,
+                initial_states,
+                mask=mask,
+                unroll=unroll,
+                input_length=num_timesteps if unroll else None)
+
+            assert_allclose(K.eval(outputs), expected_outputs)
+
+    @pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported')
+    def test_rnn_state_num_dim_larger_than_2_masking(self):
+        num_samples = 3
+        num_timesteps = 4
+
+        def step_function(inputs, states):
+            return inputs, [s + 1 for s in states]
+
+        inputs_vals = np.random.random((num_samples, num_timesteps, 5))
+        initial_state_vals = np.random.random((num_samples, 6, 7))
+        mask_vals = np.ones((num_samples, num_timesteps))
+        mask_vals[0, -2:] = 0  # final two timesteps masked for first sample
+
+        expected_last_state = initial_state_vals.copy()
+        expected_last_state[0] += (num_timesteps - 2)
+        expected_last_state[1:] += num_timesteps
+
+        inputs = K.variable(inputs_vals)
+        initial_states = [K.variable(initial_state_vals)]
+        mask = K.variable(mask_vals)
+        for unroll in [True, False]:
+            last_output, outputs, last_states = K.rnn(
+                step_function,
+                inputs,
+                initial_states,
+                mask=mask,
+                unroll=unroll,
+                input_length=num_timesteps if unroll else None)
+
+            # not updated last timestep:
+            assert_allclose(K.eval(last_states[0]), expected_last_state)
+
     @pytest.mark.parametrize('x_np,axis,keepdims', [
         (np.array([1.1, 0.8, 0.9]), 0, False),
         (np.array([[1.1, 0.8, 0.9]]), 0, False),
diff --git a/tests/keras/layers/recurrent_test.py b/tests/keras/layers/recurrent_test.py
index e6634043..729ce65c 100644
--- a/tests/keras/layers/recurrent_test.py
+++ b/tests/keras/layers/recurrent_test.py
@@ -165,6 +165,120 @@ def test_masking_correctness(layer_class):
     assert_allclose(out7, out6, atol=1e-5)
 
 
+@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')
+def test_masking_correctness_output_not_equal_to_first_state():
+
+    class Cell(keras.layers.Layer):
+
+        def __init__(self):
+            self.state_size = None
+            self.output_size = None
+            super(Cell, self).__init__()
+
+        def build(self, input_shape):
+            self.state_size = input_shape[-1]
+            self.output_size = input_shape[-1]
+
+        def call(self, inputs, states):
+            return inputs, [s + 1 for s in states]
+
+    num_samples = 5
+    num_timesteps = 4
+    state_size = input_size = 3  # also equal to `output_size`
+
+    # random inputs and state values
+    x_vals = np.random.random((num_samples, num_timesteps, input_size))
+    # last timestep masked for first sample (all zero inputs masked by Masking layer)
+    x_vals[0, -1, :] = 0
+    s_initial_vals = np.random.random((num_samples, state_size))
+
+    # final outputs equal to last inputs
+    y_vals_expected = x_vals[:, -1].copy()
+    # except for first sample, where it is equal to second to last value due to mask
+    y_vals_expected[0] = x_vals[0, -2]
+
+    s_final_vals_expected = s_initial_vals.copy()
+    # states are incremented `num_timesteps - 1` times for first sample
+    s_final_vals_expected[0] += (num_timesteps - 1)
+    # and `num_timesteps - 1` times for remaining samples
+    s_final_vals_expected[1:] += num_timesteps
+
+    for unroll in [True, False]:
+        x = Input((num_timesteps, input_size), name="x")
+        x_masked = Masking()(x)
+        s_initial = Input((state_size,), name="s_initial")
+        y, s_final = recurrent.RNN(Cell(),
+                                   return_state=True,
+                                   unroll=unroll)(x_masked, initial_state=s_initial)
+        model = Model([x, s_initial], [y, s_final])
+        model.compile(optimizer='sgd', loss='mse')
+
+        y_vals, s_final_vals = model.predict([x_vals, s_initial_vals])
+        assert_allclose(y_vals,
+                        y_vals_expected,
+                        err_msg="Unexpected output for unroll={}".format(unroll))
+        assert_allclose(s_final_vals,
+                        s_final_vals_expected,
+                        err_msg="Unexpected state for unroll={}".format(unroll))
+
+
+@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')
+def test_masking_correctness_output_size_not_equal_to_first_state_size():
+
+    class Cell(keras.layers.Layer):
+
+        def __init__(self):
+            self.state_size = None
+            self.output_size = None
+            super(Cell, self).__init__()
+
+        def build(self, input_shape):
+            self.state_size = input_shape[-1]
+            self.output_size = input_shape[-1] * 2
+
+        def call(self, inputs, states):
+            return keras.layers.concatenate([inputs] * 2), [s + 1 for s in states]
+
+    num_samples = 5
+    num_timesteps = 6
+    input_size = state_size = 7
+
+    # random inputs and state values
+    x_vals = np.random.random((num_samples, num_timesteps, input_size))
+    # last timestep masked for first sample (all zero inputs masked by Masking layer)
+    x_vals[0, -1, :] = 0
+    s_initial_vals = np.random.random((num_samples, state_size))
+
+    # final outputs equal to last inputs concatenated
+    y_vals_expected = np.concatenate([x_vals[:, -1]] * 2, axis=-1)
+    # except for first sample, where it is equal to second to last value due to mask
+    y_vals_expected[0] = np.concatenate([x_vals[0, -2]] * 2, axis=-1)
+
+    s_final_vals_expected = s_initial_vals.copy()
+    # states are incremented `num_timesteps - 1` times for first sample
+    s_final_vals_expected[0] += (num_timesteps - 1)
+    # and `num_timesteps - 1` times for remaining samples
+    s_final_vals_expected[1:] += num_timesteps
+
+    for unroll in [True, False]:
+        x = Input((num_timesteps, input_size), name="x")
+        x_masked = Masking()(x)
+        s_initial = Input((state_size,), name="s_initial")
+        y, s_final = recurrent.RNN(Cell(),
+                                   return_state=True,
+                                   unroll=unroll)(x_masked, initial_state=s_initial)
+        model = Model([x, s_initial], [y, s_final])
+        model.compile(optimizer='sgd', loss='mse')
+
+        y_vals, s_final_vals = model.predict([x_vals, s_initial_vals])
+        assert_allclose(y_vals,
+                        y_vals_expected,
+                        err_msg="Unexpected output for unroll={}".format(unroll))
+        assert_allclose(s_final_vals,
+                        s_final_vals_expected,
+                        err_msg="Unexpected state for unroll={}".format(unroll))
+
+
 @rnn_test
 def test_implementation_mode(layer_class):
     for mode in [1, 2]:
