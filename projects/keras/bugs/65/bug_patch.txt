diff --git a/keras/backend/cntk_backend.py b/keras/backend/cntk_backend.py
index 2eb05fef..9228d717 100644
--- a/keras/backend/cntk_backend.py
+++ b/keras/backend/cntk_backend.py
@@ -2039,8 +2039,8 @@ def batch_get_value(xs):
 def set_value(x, value):
     if (isinstance(x, C.variables.Parameter) or
        isinstance(x, C.variables.Constant)):
-        if isinstance(value, float):
-            value = np.full(x.shape, value)
+        if isinstance(value, (float, int)):
+            value = np.full(x.shape, value, dtype=floatx())
         x.value = value
     else:
         raise NotImplementedError
diff --git a/keras/callbacks.py b/keras/callbacks.py
index dfb3cd33..8b5ec5f3 100644
--- a/keras/callbacks.py
+++ b/keras/callbacks.py
@@ -18,6 +18,7 @@ from collections import OrderedDict
 from collections import Iterable
 from .utils.generic_utils import Progbar
 from . import backend as K
+from .engine.topology import Layer
 
 try:
     import requests
@@ -202,8 +203,20 @@ class BaseLogger(Callback):
     """Callback that accumulates epoch averages of metrics.
 
     This callback is automatically applied to every Keras model.
+
+    # Arguments
+        stateful_metrics: Iterable of string names of metrics that
+            should *not* be averaged over an epoch.
+            Metrics in this list will be logged as-is in `on_epoch_end`.
+            All others will be averaged in `on_epoch_end`.
     """
 
+    def __init__(self, stateful_metrics=None):
+        if stateful_metrics:
+            self.stateful_metrics = set(stateful_metrics)
+        else:
+            self.stateful_metrics = set()
+
     def on_epoch_begin(self, epoch, logs=None):
         self.seen = 0
         self.totals = {}
@@ -214,17 +227,23 @@ class BaseLogger(Callback):
         self.seen += batch_size
 
         for k, v in logs.items():
-            if k in self.totals:
-                self.totals[k] += v * batch_size
+            if k in self.stateful_metrics:
+                self.totals[k] = v
             else:
-                self.totals[k] = v * batch_size
+                if k in self.totals:
+                    self.totals[k] += v * batch_size
+                else:
+                    self.totals[k] = v * batch_size
 
     def on_epoch_end(self, epoch, logs=None):
         if logs is not None:
             for k in self.params['metrics']:
                 if k in self.totals:
                     # Make value available to next callbacks.
-                    logs[k] = self.totals[k] / self.seen
+                    if k in self.stateful_metrics:
+                        logs[k] = self.totals[k]
+                    else:
+                        logs[k] = self.totals[k] / self.seen
 
 
 class TerminateOnNaN(Callback):
@@ -250,12 +269,17 @@ class ProgbarLogger(Callback):
         count_mode: One of "steps" or "samples".
             Whether the progress bar should
             count samples seen or steps (batches) seen.
+        stateful_metrics: Iterable of string names of metrics that
+            should *not* be averaged over an epoch.
+            Metrics in this list will be logged as-is.
+            All others will be averaged over time (e.g. loss, etc).
 
     # Raises
         ValueError: In case of invalid `count_mode`.
     """
 
-    def __init__(self, count_mode='samples'):
+    def __init__(self, count_mode='samples',
+                 stateful_metrics=None):
         super(ProgbarLogger, self).__init__()
         if count_mode == 'samples':
             self.use_steps = False
@@ -263,6 +287,10 @@ class ProgbarLogger(Callback):
             self.use_steps = True
         else:
             raise ValueError('Unknown `count_mode`: ' + str(count_mode))
+        if stateful_metrics:
+            self.stateful_metrics = set(stateful_metrics)
+        else:
+            self.stateful_metrics = set()
 
     def on_train_begin(self, logs=None):
         self.verbose = self.params['verbose']
@@ -277,7 +305,8 @@ class ProgbarLogger(Callback):
                 target = self.params['samples']
             self.target = target
             self.progbar = Progbar(target=self.target,
-                                   verbose=self.verbose)
+                                   verbose=self.verbose,
+                                   stateful_metrics=self.stateful_metrics)
         self.seen = 0
 
     def on_batch_begin(self, batch, logs=None):
diff --git a/keras/engine/training.py b/keras/engine/training.py
index 93724e92..103b8e58 100644
--- a/keras/engine/training.py
+++ b/keras/engine/training.py
@@ -11,6 +11,7 @@ import numpy as np
 from scipy.sparse import issparse
 
 from .topology import Container
+from .topology import Layer
 from .. import backend as K
 from .. import optimizers
 from .. import losses
@@ -807,7 +808,7 @@ class Model(Container):
                 self._feed_sample_weight_modes.append(self.sample_weight_modes[i])
 
         # Prepare metrics.
-        self.metrics = metrics
+        self.metrics = metrics or []
         self.weighted_metrics = weighted_metrics
         self.metrics_names = ['loss']
         self.metrics_tensors = []
@@ -850,14 +851,8 @@ class Model(Container):
         # contains tuples (metrics for output, names of metrics).
         nested_metrics = _collect_metrics(metrics, self.output_names)
         nested_weighted_metrics = _collect_metrics(weighted_metrics, self.output_names)
-
-        def append_metric(layer_index, metric_name, metric_tensor):
-            """Helper function used in loop below."""
-            if len(self.output_names) > 1:
-                metric_name = self.output_names[layer_index] + '_' + metric_name
-            self.metrics_names.append(metric_name)
-            self.metrics_tensors.append(metric_tensor)
-
+        self.metrics_updates = []
+        self.stateful_metric_names = []
         with K.name_scope('metrics'):
             for i in range(len(self.outputs)):
                 if i in skip_target_indices:
@@ -881,37 +876,60 @@ class Model(Container):
                                self.loss_functions[i] == losses.binary_crossentropy):
                                 # case: binary accuracy/crossentropy
                                 if metric in ('accuracy', 'acc'):
-                                    acc_fn = metrics_module.binary_accuracy
+                                    metric_fn = metrics_module.binary_accuracy
                                 elif metric in ('crossentropy', 'ce'):
-                                    acc_fn = metrics_module.binary_crossentropy
+                                    metric_fn = metrics_module.binary_crossentropy
                             elif self.loss_functions[i] == losses.sparse_categorical_crossentropy:
                                 # case: categorical accuracy/crossentropy with sparse targets
                                 if metric in ('accuracy', 'acc'):
-                                    acc_fn = metrics_module.sparse_categorical_accuracy
+                                    metric_fn = metrics_module.sparse_categorical_accuracy
                                 elif metric in ('crossentropy', 'ce'):
-                                    acc_fn = metrics_module.sparse_categorical_crossentropy
+                                    metric_fn = metrics_module.sparse_categorical_crossentropy
                             else:
                                 # case: categorical accuracy/crossentropy
                                 if metric in ('accuracy', 'acc'):
-                                    acc_fn = metrics_module.categorical_accuracy
+                                    metric_fn = metrics_module.categorical_accuracy
                                 elif metric in ('crossentropy', 'ce'):
-                                    acc_fn = metrics_module.categorical_crossentropy
+                                    metric_fn = metrics_module.categorical_crossentropy
                             if metric in ('accuracy', 'acc'):
                                     suffix = 'acc'
                             elif metric in ('crossentropy', 'ce'):
                                     suffix = 'ce'
-                            weighted_metric_fn = _weighted_masked_objective(acc_fn)
+                            weighted_metric_fn = _weighted_masked_objective(metric_fn)
                             metric_name = metric_name_prefix + suffix
                         else:
                             metric_fn = metrics_module.get(metric)
                             weighted_metric_fn = _weighted_masked_objective(metric_fn)
-                            metric_name = metric_name_prefix + metric_fn.__name__
+                            # Get metric name as string
+                            if hasattr(metric_fn, 'name'):
+                                metric_name = metric_fn.name
+                            else:
+                                metric_name = metric_fn.__name__
+                            metric_name = metric_name_prefix + metric_name
 
                         with K.name_scope(metric_name):
                             metric_result = weighted_metric_fn(y_true, y_pred,
                                                                weights=weights,
                                                                mask=masks[i])
-                        append_metric(i, metric_name, metric_result)
+
+                        # Append to self.metrics_names, self.metric_tensors,
+                        # self.stateful_metric_names
+                        if len(self.output_names) > 1:
+                            metric_name = self.output_names[i] + '_' + metric_name
+                        # Dedupe name
+                        j = 1
+                        base_metric_name = metric_name
+                        while metric_name in self.metrics_names:
+                            metric_name = base_metric_name + '_' + str(j)
+                            j += 1
+                        self.metrics_names.append(metric_name)
+                        self.metrics_tensors.append(metric_result)
+
+                        # Keep track of state updates created by
+                        # stateful metrics (i.e. metrics layers).
+                        if isinstance(metric_fn, Layer):
+                            self.stateful_metric_names.append(metric_name)
+                            self.metrics_updates += metric_fn.updates
 
                 handle_metrics(output_metrics)
                 handle_metrics(output_weighted_metrics, weights=weights)
@@ -970,7 +988,7 @@ class Model(Container):
                     training_updates = self.optimizer.get_updates(
                         params=self._collected_trainable_weights,
                         loss=self.total_loss)
-                updates = self.updates + training_updates
+                updates = self.updates + training_updates + self.metrics_updates
                 # Gets loss and metrics. Updates weights at each call.
                 self.train_function = K.function(inputs,
                                                  [self.total_loss] + self.metrics_tensors,
@@ -989,7 +1007,7 @@ class Model(Container):
             # Does update the network states.
             self.test_function = K.function(inputs,
                                             [self.total_loss] + self.metrics_tensors,
-                                            updates=self.state_updates,
+                                            updates=self.state_updates + self.metrics_updates,
                                             name='test_function',
                                             **self._function_kwargs)
 
@@ -1110,14 +1128,19 @@ class Model(Container):
             index_array = np.arange(num_train_samples)
 
         self.history = cbks.History()
-        callbacks = [cbks.BaseLogger()] + (callbacks or []) + [self.history]
+        _callbacks = [cbks.BaseLogger(
+            stateful_metrics=self.stateful_metric_names)]
         if verbose:
             if steps_per_epoch is not None:
                 count_mode = 'steps'
             else:
                 count_mode = 'samples'
-            callbacks.insert(1, cbks.ProgbarLogger(count_mode))
-        callbacks = cbks.CallbackList(callbacks)
+            _callbacks.append(
+                cbks.ProgbarLogger(
+                    count_mode,
+                    stateful_metrics=self.stateful_metric_names))
+        _callbacks += (callbacks or []) + [self.history]
+        callbacks = cbks.CallbackList(_callbacks)
         out_labels = out_labels or []
 
         # it's possible to callback a different model than self
@@ -1150,6 +1173,10 @@ class Model(Container):
                 indices_for_conversion_to_dense.append(i)
 
         for epoch in range(initial_epoch, epochs):
+            # Reset stateful metrics
+            for m in self.metrics:
+                if isinstance(m, Layer):
+                    m.reset_states()
             callbacks.on_epoch_begin(epoch)
             epoch_logs = {}
             if steps_per_epoch is not None:
@@ -1248,14 +1275,21 @@ class Model(Container):
             or list of arrays of predictions
             (if the model has multiple outputs).
         """
+
+        if hasattr(self, 'metrics'):
+            for m in self.metrics:
+                if isinstance(m, Layer):
+                    m.reset_states()
         num_samples = self._check_num_samples(ins, batch_size,
                                               steps,
                                               'steps')
         if verbose == 1:
             if steps is not None:
-                progbar = Progbar(target=steps)
+                progbar = Progbar(target=steps,
+                                  stateful_metrics=self.stateful_metric_names)
             else:
-                progbar = Progbar(target=num_samples)
+                progbar = Progbar(target=num_samples,
+                                  stateful_metrics=self.stateful_metric_names)
 
         indices_for_conversion_to_dense = []
         for i in range(len(self._feed_inputs)):
@@ -1334,6 +1368,17 @@ class Model(Container):
             and/or metrics). The attribute `model.metrics_names` will give you
             the display labels for the scalar outputs.
         """
+
+        if hasattr(self, 'metrics'):
+            for m in self.metrics:
+                if isinstance(m, Layer):
+                    m.reset_states()
+            stateful_metric_indices = [
+                i for i, name in enumerate(self.metrics_names)
+                if str(name) in self.stateful_metric_names]
+        else:
+            stateful_metric_indices = []
+
         num_samples = self._check_num_samples(ins, batch_size,
                                               steps,
                                               'steps')
@@ -1359,7 +1404,10 @@ class Model(Container):
                         for _ in enumerate(batch_outs):
                             outs.append(0.)
                     for i, batch_out in enumerate(batch_outs):
-                        outs[i] += batch_out
+                        if i in stateful_metric_indices:
+                            outs[i] = batch_out
+                        else:
+                            outs[i] += batch_out
                 else:
                     if step == 0:
                         outs.append(0.)
@@ -1367,7 +1415,8 @@ class Model(Container):
                 if verbose == 1:
                     progbar.update(step + 1)
             for i in range(len(outs)):
-                outs[i] /= steps
+                if i not in stateful_metric_indices:
+                    outs[i] /= steps
         else:
             batches = _make_batches(num_samples, batch_size)
             index_array = np.arange(num_samples)
@@ -1387,7 +1436,10 @@ class Model(Container):
                         for batch_out in enumerate(batch_outs):
                             outs.append(0.)
                     for i, batch_out in enumerate(batch_outs):
-                        outs[i] += batch_out * len(batch_ids)
+                        if i in stateful_metric_indices:
+                            outs[i] = batch_out
+                        else:
+                            outs[i] += batch_out * len(batch_ids)
                 else:
                     if batch_index == 0:
                         outs.append(0.)
@@ -1396,7 +1448,8 @@ class Model(Container):
                 if verbose == 1:
                     progbar.update(batch_end)
             for i in range(len(outs)):
-                outs[i] /= num_samples
+                if i not in stateful_metric_indices:
+                    outs[i] /= num_samples
         if len(outs) == 1:
             return outs[0]
         return outs
@@ -1454,20 +1507,6 @@ class Model(Container):
                                  str(x[0].shape[0]) + ' samples')
         return x, y, sample_weights
 
-    def _get_deduped_metrics_names(self):
-        out_labels = self.metrics_names
-
-        # Rename duplicated metrics name
-        # (can happen with an output layer shared among multiple dataflows).
-        deduped_out_labels = []
-        for i, label in enumerate(out_labels):
-            new_label = label
-            if out_labels.count(label) > 1:
-                dup_idx = out_labels[:i].count(label)
-                new_label += '_' + str(dup_idx + 1)
-            deduped_out_labels.append(new_label)
-        return deduped_out_labels
-
     def fit(self,
             x=None,
             y=None,
@@ -1651,7 +1690,7 @@ class Model(Container):
         f = self.train_function
 
         # Prepare display labels.
-        out_labels = self._get_deduped_metrics_names()
+        out_labels = self.metrics_names
 
         if do_validation:
             self._make_test_function()
@@ -2079,15 +2118,20 @@ class Model(Container):
                              ' the `keras.utils.Sequence` class.')
 
         # Prepare display labels.
-        out_labels = self._get_deduped_metrics_names()
+        out_labels = self.metrics_names
         callback_metrics = out_labels + ['val_' + n for n in out_labels]
 
         # prepare callbacks
         self.history = cbks.History()
-        callbacks = [cbks.BaseLogger()] + (callbacks or []) + [self.history]
+        _callbacks = [cbks.BaseLogger(
+            stateful_metrics=self.stateful_metric_names)]
         if verbose:
-            callbacks.insert(1, cbks.ProgbarLogger(count_mode='steps'))
-        callbacks = cbks.CallbackList(callbacks)
+            _callbacks.append(
+                cbks.ProgbarLogger(
+                    count_mode='steps',
+                    stateful_metrics=self.stateful_metric_names))
+        _callbacks += (callbacks or []) + [self.history]
+        callbacks = cbks.CallbackList(_callbacks)
 
         # it's possible to callback a different model than self:
         if hasattr(self, 'callback_model') and self.callback_model:
diff --git a/keras/metrics.py b/keras/metrics.py
index bcc625bc..3d5df23b 100644
--- a/keras/metrics.py
+++ b/keras/metrics.py
@@ -20,6 +20,7 @@ from .losses import kullback_leibler_divergence
 from .losses import poisson
 from .losses import cosine_proximity
 from .utils.generic_utils import deserialize_keras_object
+from .utils.generic_utils import serialize_keras_object
 
 
 def binary_accuracy(y_true, y_pred):
@@ -56,20 +57,22 @@ cosine = cosine_proximity
 
 
 def serialize(metric):
-    return metric.__name__
+    return serialize_keras_object(metric)
 
 
-def deserialize(name, custom_objects=None):
-    return deserialize_keras_object(name,
+def deserialize(config, custom_objects=None):
+    return deserialize_keras_object(config,
                                     module_objects=globals(),
                                     custom_objects=custom_objects,
                                     printable_module_name='metric function')
 
 
 def get(identifier):
-    if isinstance(identifier, six.string_types):
-        identifier = str(identifier)
-        return deserialize(identifier)
+    if isinstance(identifier, dict):
+        config = {'class_name': str(identifier), 'config': {}}
+        return deserialize(config)
+    elif isinstance(identifier, six.string_types):
+        return deserialize(str(identifier))
     elif callable(identifier):
         return identifier
     else:
diff --git a/keras/utils/generic_utils.py b/keras/utils/generic_utils.py
index c3e07701..c54f6a75 100644
--- a/keras/utils/generic_utils.py
+++ b/keras/utils/generic_utils.py
@@ -13,6 +13,7 @@ import marshal
 import types as python_types
 import inspect
 import codecs
+import collections
 
 _GLOBAL_CUSTOM_OBJECTS = {}
 
@@ -286,23 +287,34 @@ class Progbar(object):
 
     # Arguments
         target: Total number of steps expected, None if unknown.
+        width: Progress bar width on screen.
+        verbose: Verbosity mode, 0 (silent), 1 (verbose), 2 (semi-verbose)
+        stateful_metrics: Iterable of string names of metrics that
+            should *not* be averaged over time. Metrics in this list
+            will be displayed as-is. All others will be averaged
+            by the progbar before display.
         interval: Minimum visual progress update interval (in seconds).
     """
 
-    def __init__(self, target, width=30, verbose=1, interval=0.05):
-        self.width = width
+    def __init__(self, target, width=30, verbose=1, interval=0.05,
+                 stateful_metrics=None):
         self.target = target
-        self.sum_values = {}
-        self.unique_values = []
-        self.start = time.time()
-        self.last_update = 0
-        self.interval = interval
-        self.total_width = 0
-        self.seen_so_far = 0
+        self.width = width
         self.verbose = verbose
+        self.interval = interval
+        if stateful_metrics:
+            self.stateful_metrics = set(stateful_metrics)
+        else:
+            self.stateful_metrics = set()
+
         self._dynamic_display = ((hasattr(sys.stdout, 'isatty') and
                                   sys.stdout.isatty()) or
                                  'ipykernel' in sys.modules)
+        self._total_width = 0
+        self._seen_so_far = 0
+        self._values = collections.OrderedDict()
+        self._start = time.time()
+        self._last_update = 0
 
     def update(self, current, values=None):
         """Updates the progress bar.
@@ -311,27 +323,31 @@ class Progbar(object):
             current: Index of current step.
             values: List of tuples:
                 `(name, value_for_last_step)`.
-                The progress bar will display averages for these values.
+                If `name` is in `stateful_metrics`,
+                `value_for_last_step` will be displayed as-is.
+                Else, an average of the metric over time will be displayed.
         """
         values = values or []
         for k, v in values:
-            if k not in self.sum_values:
-                self.sum_values[k] = [v * (current - self.seen_so_far),
-                                      current - self.seen_so_far]
-                self.unique_values.append(k)
+            if k not in self.stateful_metrics:
+                if k not in self._values:
+                    self._values[k] = [v * (current - self._seen_so_far),
+                                       current - self._seen_so_far]
+                else:
+                    self._values[k][0] += v * (current - self._seen_so_far)
+                    self._values[k][1] += (current - self._seen_so_far)
             else:
-                self.sum_values[k][0] += v * (current - self.seen_so_far)
-                self.sum_values[k][1] += (current - self.seen_so_far)
-        self.seen_so_far = current
+                self._values[k] = v
+        self._seen_so_far = current
 
         now = time.time()
-        info = ' - %.0fs' % (now - self.start)
+        info = ' - %.0fs' % (now - self._start)
         if self.verbose == 1:
-            if (now - self.last_update < self.interval and
+            if (now - self._last_update < self.interval and
                     self.target is not None and current < self.target):
                 return
 
-            prev_total_width = self.total_width
+            prev_total_width = self._total_width
             if self._dynamic_display:
                 sys.stdout.write('\b' * prev_total_width)
                 sys.stdout.write('\r')
@@ -355,11 +371,11 @@ class Progbar(object):
             else:
                 bar = '%7d/Unknown' % current
 
-            self.total_width = len(bar)
+            self._total_width = len(bar)
             sys.stdout.write(bar)
 
             if current:
-                time_per_unit = (now - self.start) / current
+                time_per_unit = (now - self._start) / current
             else:
                 time_per_unit = 0
             if self.target is not None and current < self.target:
@@ -380,21 +396,21 @@ class Progbar(object):
                 else:
                     info += ' %.0fus/step' % (time_per_unit * 1e6)
 
-            for k in self.unique_values:
+            for k in self._values:
                 info += ' - %s:' % k
-                if isinstance(self.sum_values[k], list):
+                if isinstance(self._values[k], list):
                     avg = np.mean(
-                        self.sum_values[k][0] / max(1, self.sum_values[k][1]))
+                        self._values[k][0] / max(1, self._values[k][1]))
                     if abs(avg) > 1e-3:
                         info += ' %.4f' % avg
                     else:
                         info += ' %.4e' % avg
                 else:
-                    info += ' %s' % self.sum_values[k]
+                    info += ' %s' % self._values[k]
 
-            self.total_width += len(info)
-            if prev_total_width > self.total_width:
-                info += (' ' * (prev_total_width - self.total_width))
+            self._total_width += len(info)
+            if prev_total_width > self._total_width:
+                info += (' ' * (prev_total_width - self._total_width))
 
             if self.target is not None and current >= self.target:
                 info += '\n'
@@ -404,10 +420,10 @@ class Progbar(object):
 
         elif self.verbose == 2:
             if self.target is None or current >= self.target:
-                for k in self.unique_values:
+                for k in self._values:
                     info += ' - %s:' % k
                     avg = np.mean(
-                        self.sum_values[k][0] / max(1, self.sum_values[k][1]))
+                        self._values[k][0] / max(1, self._values[k][1]))
                     if avg > 1e-3:
                         info += ' %.4f' % avg
                     else:
@@ -417,7 +433,7 @@ class Progbar(object):
                 sys.stdout.write(info)
                 sys.stdout.flush()
 
-        self.last_update = now
+        self._last_update = now
 
     def add(self, n, values=None):
-        self.update(self.seen_so_far + n, values)
+        self.update(self._seen_so_far + n, values)
