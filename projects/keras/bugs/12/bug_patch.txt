diff --git a/keras/callbacks.py b/keras/callbacks.py
index 01f68cbf..30acf0c0 100644
--- a/keras/callbacks.py
+++ b/keras/callbacks.py
@@ -17,6 +17,7 @@ import io
 from collections import deque
 from collections import OrderedDict
 from collections import Iterable
+from collections import defaultdict
 from .utils.generic_utils import Progbar
 from . import backend as K
 from .engine.training_utils import standardize_input_data
@@ -27,6 +28,11 @@ except ImportError:
     requests = None
 
 
+_TRAIN = 'train'
+_TEST = 'test'
+_PREDICT = 'predict'
+
+
 class CallbackList(object):
     """Container abstracting a list of callbacks.
 
@@ -40,107 +46,227 @@ class CallbackList(object):
         callbacks = callbacks or []
         self.callbacks = [c for c in callbacks]
         self.queue_length = queue_length
+        self.params = {}
+        self.model = None
+        self._reset_batch_timing()
+
+    def _reset_batch_timing(self):
+        self._delta_t_batch = 0.
+        self._delta_ts = defaultdict(lambda: deque([], maxlen=self.queue_length))
 
     def append(self, callback):
         self.callbacks.append(callback)
 
     def set_params(self, params):
+        self.params = params
         for callback in self.callbacks:
             callback.set_params(params)
 
     def set_model(self, model):
+        self.model = model
         for callback in self.callbacks:
             callback.set_model(model)
 
+    def _call_batch_hook(self, mode, hook, batch, logs=None):
+        """Helper function for all batch_{begin | end} methods."""
+        if not self.callbacks:
+            return
+        hook_name = 'on_{mode}_batch_{hook}'.format(mode=mode, hook=hook)
+        if hook == 'end':
+            if not hasattr(self, '_t_enter_batch'):
+                self._t_enter_batch = time.time()
+            # Batch is ending, calculate batch time
+            self._delta_t_batch = time.time() - self._t_enter_batch
+
+        logs = logs or {}
+        t_before_callbacks = time.time()
+        for callback in self.callbacks:
+            batch_hook = getattr(callback, hook_name)
+            batch_hook(batch, logs)
+        self._delta_ts[hook_name].append(time.time() - t_before_callbacks)
+
+        delta_t_median = np.median(self._delta_ts[hook_name])
+        if (self._delta_t_batch > 0. and
+           delta_t_median > 0.95 * self._delta_t_batch and
+           delta_t_median > 0.1):
+            warnings.warn(
+                'Method (%s) is slow compared '
+                'to the batch update (%f). Check your callbacks.', hook_name,
+                delta_t_median)
+        if hook == 'begin':
+            self._t_enter_batch = time.time()
+
+    def _call_begin_hook(self, mode):
+        """Helper function for on_{train|test|predict}_begin methods."""
+        if mode == _TRAIN:
+            self.on_train_begin()
+        elif mode == _TEST:
+            self.on_test_begin()
+        else:
+            self.on_predict_begin()
+
+    def _call_end_hook(self, mode):
+        """Helper function for on_{train|test|predict}_end methods."""
+        if mode == _TRAIN:
+            self.on_train_end()
+        elif mode == _TEST:
+            self.on_test_end()
+        else:
+            self.on_predict_end()
+
+    def on_batch_begin(self, batch, logs=None):
+        self._call_batch_hook(_TRAIN, 'begin', batch, logs=logs)
+
+    def on_batch_end(self, batch, logs=None):
+        self._call_batch_hook(_TRAIN, 'end', batch, logs=logs)
+
     def on_epoch_begin(self, epoch, logs=None):
-        """Called at the start of an epoch.
+        """Calls the `on_epoch_begin` methods of its callbacks.
+
+        This function should only be called during train mode.
 
         # Arguments
             epoch: integer, index of epoch.
-            logs: dictionary of logs.
+            logs: dict, Currently no data is passed to this argument for this method
+                but that may change in the future.
         """
         logs = logs or {}
         for callback in self.callbacks:
             callback.on_epoch_begin(epoch, logs)
-        self._delta_t_batch = 0.
-        self._delta_ts_batch_begin = deque([], maxlen=self.queue_length)
-        self._delta_ts_batch_end = deque([], maxlen=self.queue_length)
+        self._reset_batch_timing()
 
     def on_epoch_end(self, epoch, logs=None):
-        """Called at the end of an epoch.
+        """Calls the `on_epoch_end` methods of its callbacks.
+
+        This function should only be called during train mode.
 
         # Arguments
             epoch: integer, index of epoch.
-            logs: dictionary of logs.
+            logs: dict, metric results for this training epoch, and for the
+                validation epoch if validation is performed. Validation result keys
+                are prefixed with `val_`.
         """
         logs = logs or {}
         for callback in self.callbacks:
             callback.on_epoch_end(epoch, logs)
 
-    def on_batch_begin(self, batch, logs=None):
-        """Called right before processing a batch.
+    def on_train_batch_begin(self, batch, logs=None):
+        """Calls the `on_train_batch_begin` methods of its callbacks.
 
         # Arguments
             batch: integer, index of batch within the current epoch.
-            logs: dictionary of logs.
+            logs: dict, has keys `batch` and `size` representing the current
+                batch number and the size of the batch.
         """
-        logs = logs or {}
-        t_before_callbacks = time.time()
-        for callback in self.callbacks:
-            callback.on_batch_begin(batch, logs)
-        self._delta_ts_batch_begin.append(time.time() - t_before_callbacks)
-        delta_t_median = np.median(self._delta_ts_batch_begin)
-        if (self._delta_t_batch > 0. and
-           delta_t_median > 0.95 * self._delta_t_batch and
-           delta_t_median > 0.1):
-            warnings.warn('Method on_batch_begin() is slow compared '
-                          'to the batch update (%f). Check your callbacks.'
-                          % delta_t_median)
-        self._t_enter_batch = time.time()
+        self._call_batch_hook(_TRAIN, 'begin', batch, logs=logs)
 
-    def on_batch_end(self, batch, logs=None):
-        """Called at the end of a batch.
+    def on_train_batch_end(self, batch, logs=None):
+        """Calls the `on_train_batch_end` methods of its callbacks.
 
         # Arguments
             batch: integer, index of batch within the current epoch.
-            logs: dictionary of logs.
+            logs: dict, metric results for this batch.
         """
-        logs = logs or {}
-        if not hasattr(self, '_t_enter_batch'):
-            self._t_enter_batch = time.time()
-        self._delta_t_batch = time.time() - self._t_enter_batch
-        t_before_callbacks = time.time()
-        for callback in self.callbacks:
-            callback.on_batch_end(batch, logs)
-        self._delta_ts_batch_end.append(time.time() - t_before_callbacks)
-        delta_t_median = np.median(self._delta_ts_batch_end)
-        if (self._delta_t_batch > 0. and
-           (delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1)):
-            warnings.warn('In your callbacks, method `on_batch_end()` '
-                          'is slow compared to a model step '
-                          '(%f vs %f). Check your callbacks.'
-                          % (delta_t_median, self._delta_t_batch))
+        self._call_batch_hook(_TRAIN, 'end', batch, logs=logs)
+
+    def on_test_batch_begin(self, batch, logs=None):
+        """Calls the `on_test_batch_begin` methods of its callbacks.
+
+        # Arguments
+            batch: integer, index of batch within the current epoch.
+            logs: dict, has keys `batch` and `size` representing the current
+                batch number and the size of the batch.
+        """
+        self._call_batch_hook(_TEST, 'begin', batch, logs=logs)
+
+    def on_test_batch_end(self, batch, logs=None):
+        """Calls the `on_test_batch_end` methods of its callbacks.
+
+        # Arguments
+            batch: integer, index of batch within the current epoch.
+            logs: dict, metric results for this batch.
+        """
+        self._call_batch_hook(_TEST, 'end', batch, logs=logs)
+
+    def on_predict_batch_begin(self, batch, logs=None):
+        """Calls the `on_predict_batch_begin` methods of its callbacks.
+
+        # Arguments
+            batch: integer, index of batch within the current epoch.
+            logs: dict, has keys `batch` and `size` representing the current
+                batch number and the size of the batch.
+        """
+        self._call_batch_hook(_PREDICT, 'begin', batch, logs=logs)
+
+    def on_predict_batch_end(self, batch, logs=None):
+        """Calls the `on_predict_batch_end` methods of its callbacks.
+
+        # Argument
+            batch: integer, index of batch within the current epoch.
+            logs: dict, metric results for this batch.
+        """
+        self._call_batch_hook(_PREDICT, 'end', batch, logs=logs)
 
     def on_train_begin(self, logs=None):
-        """Called at the beginning of training.
+        """Calls the `on_train_begin` methods of its callbacks.
 
         # Arguments
-            logs: dictionary of logs.
+            logs: dict, currently no data is passed to this argument for this method
+                but that may change in the future.
         """
-        logs = logs or {}
         for callback in self.callbacks:
             callback.on_train_begin(logs)
 
     def on_train_end(self, logs=None):
-        """Called at the end of training.
+        """Calls the `on_train_end` methods of its callbacks.
 
         # Arguments
-            logs: dictionary of logs.
+            logs: dict, currently no data is passed to this argument for this method
+                but that may change in the future.
         """
-        logs = logs or {}
         for callback in self.callbacks:
             callback.on_train_end(logs)
 
+    def on_test_begin(self, logs=None):
+        """Calls the `on_test_begin` methods of its callbacks.
+
+        # Arguments
+            logs: dict, currently no data is passed to this argument for this method
+                but that may change in the future.
+        """
+        for callback in self.callbacks:
+            callback.on_test_begin(logs)
+
+    def on_test_end(self, logs=None):
+        """Calls the `on_test_end` methods of its callbacks.
+
+        # Arguments
+            logs: dict, currently no data is passed to this argument for this method
+                but that may change in the future.
+        """
+        for callback in self.callbacks:
+            callback.on_test_end(logs)
+
+    def on_predict_begin(self, logs=None):
+        """Calls the `on_predict_begin` methods of its callbacks.
+
+        # Arguments
+            logs: dict, currently no data is passed to this argument for this method
+                but that may change in the future.
+        """
+        for callback in self.callbacks:
+            callback.on_predict_begin(logs)
+
+    def on_predict_end(self, logs=None):
+        """Calls the `on_predict_end` methods of its callbacks.
+
+        # Arguments
+            logs: dict, currently no data is passed to this argument for this method
+                but that may change in the future.
+        """
+        for callback in self.callbacks:
+            callback.on_predict_end(logs)
+
     def __iter__(self):
         return iter(self.callbacks)
 
@@ -182,23 +308,169 @@ class Callback(object):
     def set_model(self, model):
         self.model = model
 
+    def on_batch_begin(self, batch, logs=None):
+        """A backwards compatibility alias for `on_train_batch_begin`."""
+
+    def on_batch_end(self, batch, logs=None):
+        """A backwards compatibility alias for `on_train_batch_end`."""
+
     def on_epoch_begin(self, epoch, logs=None):
-        pass
+        """Called at the start of an epoch.
+
+        Subclasses should override for any actions to run. This function should only
+        be called during train mode.
+
+        # Arguments
+            epoch: integer, index of epoch.
+            logs: dict, currently no data is passed to this argument for this method
+                but that may change in the future.
+        """
 
     def on_epoch_end(self, epoch, logs=None):
-        pass
+        """Called at the end of an epoch.
 
-    def on_batch_begin(self, batch, logs=None):
-        pass
+        Subclasses should override for any actions to run. This function should only
+        be called during train mode.
 
-    def on_batch_end(self, batch, logs=None):
-        pass
+        # Arguments
+            epoch: integer, index of epoch.
+            logs: dict, metric results for this training epoch, and for the
+                validation epoch if validation is performed. Validation result keys
+                are prefixed with `val_`.
+        """
+
+    def on_train_batch_begin(self, batch, logs=None):
+        """Called at the beginning of a training batch in `fit` methods.
+
+        Subclasses should override for any actions to run.
+
+        # Arguments
+            batch: integer, index of batch within the current epoch.
+            logs: dict, has keys `batch` and `size` representing the current
+                batch number and the size of the batch.
+        """
+        # For backwards compatibility
+        self.on_batch_begin(batch, logs=logs)
+
+    def on_train_batch_end(self, batch, logs=None):
+        """Called at the end of a training batch in `fit` methods.
+
+        Subclasses should override for any actions to run.
+
+        # Arguments
+            batch: integer, index of batch within the current epoch.
+            logs: dict, metric results for this batch.
+        """
+        # For backwards compatibility
+        self.on_batch_end(batch, logs=logs)
+
+    def on_test_batch_begin(self, batch, logs=None):
+        """Called at the beginning of a batch in `evaluate` methods.
+
+        Also called at the beginning of a validation batch in the `fit` methods,
+        if validation data is provided.
+
+        Subclasses should override for any actions to run.
+
+        # Arguments
+            batch: integer, index of batch within the current epoch.
+            logs: dict, has keys `batch` and `size` representing the current
+                batch number and the size of the batch.
+        """
+
+    def on_test_batch_end(self, batch, logs=None):
+        """Called at the end of a batch in `evaluate` methods.
+
+        Also called at the end of a validation batch in the `fit` methods,
+        if validation data is provided.
+
+        Subclasses should override for any actions to run.
+
+        # Arguments
+            batch: integer, index of batch within the current epoch.
+            logs: dict, metric results for this batch.
+        """
+
+    def on_predict_batch_begin(self, batch, logs=None):
+        """Called at the beginning of a batch in `predict` methods.
+
+        Subclasses should override for any actions to run.
+
+        # Arguments
+            batch: integer, index of batch within the current epoch.
+            logs: dict, has keys `batch` and `size` representing the current
+                batch number and the size of the batch.
+        """
+
+    def on_predict_batch_end(self, batch, logs=None):
+        """Called at the end of a batch in `predict` methods.
+
+        Subclasses should override for any actions to run.
+
+        # Arguments
+            batch: integer, index of batch within the current epoch.
+            logs: dict, metric results for this batch.
+        """
 
     def on_train_begin(self, logs=None):
-        pass
+        """Called at the beginning of training.
+
+        Subclasses should override for any actions to run.
+
+        # Arguments
+            logs: dict, currently no data is passed to this argument for this method
+                but that may change in the future.
+        """
 
     def on_train_end(self, logs=None):
-        pass
+        """Called at the end of training.
+
+        Subclasses should override for any actions to run.
+
+        # Arguments
+            logs: dict, currently no data is passed to this argument for this method
+                but that may change in the future.
+        """
+
+    def on_test_begin(self, logs=None):
+        """Called at the beginning of evaluation or validation.
+
+        Subclasses should override for any actions to run.
+
+        # Arguments
+            logs: dict, currently no data is passed to this argument for this method
+                but that may change in the future.
+        """
+
+    def on_test_end(self, logs=None):
+        """Called at the end of evaluation or validation.
+
+        Subclasses should override for any actions to run.
+
+        # Arguments
+            logs: dict, currently no data is passed to this argument for this method
+                but that may change in the future.
+        """
+
+    def on_predict_begin(self, logs=None):
+        """Called at the beginning of prediction.
+
+        Subclasses should override for any actions to run.
+
+        # Arguments
+            logs: dict, currently no data is passed to this argument for this method
+                but that may change in the future.
+        """
+
+    def on_predict_end(self, logs=None):
+        """Called at the end of prediction.
+
+        Subclasses should override for any actions to run.
+
+        # Arguments
+            logs: dict, currently no data is passed to this argument for this method
+                but that may change in the future.
+        """
 
 
 class BaseLogger(Callback):
diff --git a/keras/engine/training.py b/keras/engine/training.py
index 6f0d6e9b..0ccfb572 100644
--- a/keras/engine/training.py
+++ b/keras/engine/training.py
@@ -823,6 +823,12 @@ class Model(Network):
                                  str(x[0].shape[0]) + ' samples')
         return x, y, sample_weights
 
+    def _get_callback_model(self):
+        """Returns the Callback Model for this Model."""
+        if hasattr(self, 'callback_model') and self.callback_model:
+            return self.callback_model
+        return self
+
     def fit(self,
             x=None,
             y=None,
@@ -869,7 +875,8 @@ class Model(Network):
             verbose: Integer. 0, 1, or 2. Verbosity mode.
                 0 = silent, 1 = progress bar, 2 = one line per epoch.
             callbacks: List of `keras.callbacks.Callback` instances.
-                List of callbacks to apply during training.
+                List of callbacks to apply during training and validation
+                (if ).
                 See [callbacks](/callbacks).
             validation_split: Float between 0 and 1.
                 Fraction of the training data to be used as validation data.
@@ -1043,7 +1050,8 @@ class Model(Network):
                  batch_size=None,
                  verbose=1,
                  sample_weight=None,
-                 steps=None):
+                 steps=None,
+                 callbacks=None):
         """Returns the loss value & metrics values for the model in test mode.
 
         Computation is done in batches.
@@ -1082,6 +1090,9 @@ class Model(Network):
                 Total number of steps (batches of samples)
                 before declaring the evaluation round finished.
                 Ignored with the default value of `None`.
+            callbacks: List of `keras.callbacks.Callback` instances.
+                List of callbacks to apply during evaluation.
+                See [callbacks](/callbacks).
 
         # Returns
             Scalar test loss (if the model has a single output and no metrics)
@@ -1111,12 +1122,14 @@ class Model(Network):
         return training_arrays.test_loop(self, f, ins,
                                          batch_size=batch_size,
                                          verbose=verbose,
-                                         steps=steps)
+                                         steps=steps,
+                                         callbacks=callbacks)
 
     def predict(self, x,
                 batch_size=None,
                 verbose=0,
-                steps=None):
+                steps=None,
+                callbacks=None):
         """Generates output predictions for the input samples.
 
         Computation is done in batches.
@@ -1129,6 +1142,9 @@ class Model(Network):
             steps: Total number of steps (batches of samples)
                 before declaring the prediction round finished.
                 Ignored with the default value of `None`.
+            callbacks: List of `keras.callbacks.Callback` instances.
+                List of callbacks to apply during prediction.
+                See [callbacks](/callbacks).
 
         # Returns
             Numpy array(s) of predictions.
@@ -1167,7 +1183,8 @@ class Model(Network):
         return training_arrays.predict_loop(self, f, ins,
                                             batch_size=batch_size,
                                             verbose=verbose,
-                                            steps=steps)
+                                            steps=steps,
+                                            callbacks=callbacks)
 
     def train_on_batch(self, x, y,
                        sample_weight=None,
@@ -1421,6 +1438,7 @@ class Model(Network):
     @interfaces.legacy_generator_methods_support
     def evaluate_generator(self, generator,
                            steps=None,
+                           callbacks=None,
                            max_queue_size=10,
                            workers=1,
                            use_multiprocessing=False,
@@ -1440,6 +1458,9 @@ class Model(Network):
                 to yield from `generator` before stopping.
                 Optional for `Sequence`: if unspecified, will use
                 the `len(generator)` as a number of steps.
+            callbacks: List of `keras.callbacks.Callback` instances.
+                List of callbacks to apply during training.
+                See [callbacks](/callbacks).
             max_queue_size: maximum size for the generator queue
             workers: Integer. Maximum number of processes to spin up
                 when using process based threading.
@@ -1467,6 +1488,7 @@ class Model(Network):
         return training_generator.evaluate_generator(
             self, generator,
             steps=steps,
+            callbacks=callbacks,
             max_queue_size=max_queue_size,
             workers=workers,
             use_multiprocessing=use_multiprocessing,
@@ -1475,6 +1497,7 @@ class Model(Network):
     @interfaces.legacy_generator_methods_support
     def predict_generator(self, generator,
                           steps=None,
+                          callbacks=None,
                           max_queue_size=10,
                           workers=1,
                           use_multiprocessing=False,
@@ -1493,6 +1516,9 @@ class Model(Network):
                 to yield from `generator` before stopping.
                 Optional for `Sequence`: if unspecified, will use
                 the `len(generator)` as a number of steps.
+            callbacks: List of `keras.callbacks.Callback` instances.
+                List of callbacks to apply during training.
+                See [callbacks](/callbacks).
             max_queue_size: Maximum size for the generator queue.
             workers: Integer. Maximum number of processes to spin up
                 when using process based threading.
@@ -1517,6 +1543,7 @@ class Model(Network):
         return training_generator.predict_generator(
             self, generator,
             steps=steps,
+            callbacks=callbacks,
             max_queue_size=max_queue_size,
             workers=workers,
             use_multiprocessing=use_multiprocessing,
diff --git a/keras/engine/training_arrays.py b/keras/engine/training_arrays.py
index 80fcdeaf..0250a9cd 100644
--- a/keras/engine/training_arrays.py
+++ b/keras/engine/training_arrays.py
@@ -44,7 +44,8 @@ def fit_loop(model, fit_function, fit_inputs,
         batch_size: Integer batch size or None if unknown.
         epochs: Number of times to iterate over the data
         verbose: Verbosity mode, 0, 1 or 2
-        callbacks: List of callbacks to be called during training
+        callbacks: List of callbacks to be called during training and validation
+            (if `val_function` and `val_inputs` are not `None`).
         val_function: Keras function to call for validation
         val_inputs: List of tensors to be fed to `val_function`
         shuffle: Whether to shuffle the data at the beginning of each epoch
@@ -110,10 +111,7 @@ def fit_loop(model, fit_function, fit_inputs,
 
     # it's possible to callback a different model than itself
     # (used by Sequential models)
-    if hasattr(model, 'callback_model') and model.callback_model:
-        callback_model = model.callback_model
-    else:
-        callback_model = model
+    callback_model = model._get_callback_model()
 
     callbacks.set_model(callback_model)
     callbacks.set_params({
@@ -125,8 +123,8 @@ def fit_loop(model, fit_function, fit_inputs,
         'do_validation': do_validation,
         'metrics': callback_metrics or [],
     })
-    callbacks.on_train_begin()
-    callback_model.stop_training = False
+    callbacks._call_begin_hook('train')
+    callbacks.model.stop_training = False
     for cbk in callbacks:
         cbk.validation_data = val_inputs
 
@@ -148,23 +146,22 @@ def fit_loop(model, fit_function, fit_inputs,
         epoch_logs = {}
         if steps_per_epoch is not None:
             for step_index in range(steps_per_epoch):
-                batch_logs = {}
-                batch_logs['batch'] = step_index
-                batch_logs['size'] = 1
-                callbacks.on_batch_begin(step_index, batch_logs)
+                batch_logs = {'batch': step_index, 'size': 1}
+                callbacks._call_batch_hook('train', 'begin', step_index, batch_logs)
                 outs = fit_function(fit_inputs)
 
                 outs = to_list(outs)
                 for l, o in zip(out_labels, outs):
                     batch_logs[l] = o
 
-                callbacks.on_batch_end(step_index, batch_logs)
+                callbacks._call_batch_hook('train', 'end', step_index, batch_logs)
                 if callback_model.stop_training:
                     break
 
             if do_validation:
                 val_outs = test_loop(model, val_function, val_inputs,
                                      steps=validation_steps,
+                                     callbacks=callbacks,
                                      verbose=0)
                 val_outs = to_list(val_outs)
                 # Same labels assumed.
@@ -190,10 +187,8 @@ def fit_loop(model, fit_function, fit_inputs,
                     raise TypeError('TypeError while preparing batch. '
                                     'If using HDF5 input data, '
                                     'pass shuffle="batch".')
-                batch_logs = {}
-                batch_logs['batch'] = batch_index
-                batch_logs['size'] = len(batch_ids)
-                callbacks.on_batch_begin(batch_index, batch_logs)
+                batch_logs = {'batch': batch_index, 'size': len(batch_ids)}
+                callbacks._call_batch_hook('train', 'begin', batch_index, batch_logs)
                 for i in indices_for_conversion_to_dense:
                     ins_batch[i] = ins_batch[i].toarray()
 
@@ -202,27 +197,32 @@ def fit_loop(model, fit_function, fit_inputs,
                 for l, o in zip(out_labels, outs):
                     batch_logs[l] = o
 
-                callbacks.on_batch_end(batch_index, batch_logs)
-                if callback_model.stop_training:
+                callbacks._call_batch_hook('train', 'end', batch_index, batch_logs)
+                if callbacks.model.stop_training:
                     break
 
                 if batch_index == len(batches) - 1:  # Last batch.
                     if do_validation:
                         val_outs = test_loop(model, val_function, val_inputs,
                                              batch_size=batch_size,
+                                             callbacks=callbacks,
                                              verbose=0)
                         val_outs = to_list(val_outs)
                         # Same labels assumed.
                         for l, o in zip(out_labels, val_outs):
                             epoch_logs['val_' + l] = o
         callbacks.on_epoch_end(epoch, epoch_logs)
-        if callback_model.stop_training:
+        if callbacks.model.stop_training:
             break
-    callbacks.on_train_end()
+    callbacks._call_end_hook('train')
     return model.history
 
 
-def predict_loop(model, f, ins, batch_size=32, verbose=0, steps=None):
+def predict_loop(model, f, ins,
+                 batch_size=32,
+                 verbose=0,
+                 steps=None,
+                 callbacks=None):
     """Abstract method to loop over some data in batches.
 
     # Arguments
@@ -234,6 +234,8 @@ def predict_loop(model, f, ins, batch_size=32, verbose=0, steps=None):
         steps: Total number of steps (batches of samples)
             before declaring `predict_loop` finished.
             Ignored with the default value of `None`.
+        callbacks: List of callbacks or an instance of
+            `keras.callbacks.CallbackList` to be called during prediction.
 
     # Returns
         Array of predictions (if the model has a single output)
@@ -244,6 +246,20 @@ def predict_loop(model, f, ins, batch_size=32, verbose=0, steps=None):
                                     batch_size=batch_size,
                                     steps=steps,
                                     steps_name='steps')
+
+    # Check if callbacks have not been already configured
+    if not isinstance(callbacks, cbks.CallbackList):
+        callbacks = cbks.CallbackList(callbacks)
+        callback_model = model._get_callback_model()
+        callbacks.set_model(callback_model)
+        callback_params = {
+            'batch_size': batch_size,
+            'steps': steps,
+            'samples': num_samples,
+            'verbose': verbose,
+        }
+        callbacks.set_params(callback_params)
+
     if verbose == 1:
         if steps is not None:
             progbar = Progbar(target=steps)
@@ -255,6 +271,9 @@ def predict_loop(model, f, ins, batch_size=32, verbose=0, steps=None):
         if issparse(ins[i]) and not K.is_sparse(model._feed_inputs[i]):
             indices_for_conversion_to_dense.append(i)
 
+    callbacks.model.stop_training = False
+    callbacks._call_begin_hook('predict')
+
     if steps is not None:
         # Step-based predictions.
         # Since we do not know how many samples
@@ -264,6 +283,8 @@ def predict_loop(model, f, ins, batch_size=32, verbose=0, steps=None):
         # and concatenate them upon returning.
         unconcatenated_outs = []
         for step in range(steps):
+            batch_logs = {'batch': step, 'size': 1}
+            callbacks._call_batch_hook('predict', 'begin', step, batch_logs)
             batch_outs = f(ins)
             batch_outs = to_list(batch_outs)
             if step == 0:
@@ -271,8 +292,12 @@ def predict_loop(model, f, ins, batch_size=32, verbose=0, steps=None):
                     unconcatenated_outs.append([])
             for i, batch_out in enumerate(batch_outs):
                 unconcatenated_outs[i].append(batch_out)
+
+            batch_logs['outputs'] = batch_outs
+            callbacks._call_batch_hook('predict', 'end', step, batch_logs)
             if verbose == 1:
                 progbar.update(step + 1)
+        callbacks.on_predict_end()
         if len(unconcatenated_outs) == 1:
             return np.concatenate(unconcatenated_outs[0], axis=0)
         return [np.concatenate(unconcatenated_outs[i], axis=0)
@@ -292,6 +317,8 @@ def predict_loop(model, f, ins, batch_size=32, verbose=0, steps=None):
             for i in indices_for_conversion_to_dense:
                 ins_batch[i] = ins_batch[i].toarray()
 
+            batch_logs = {'batch': batch_index, 'size': len(batch_ids)}
+            callbacks._call_batch_hook('predict', 'begin', batch_index, batch_logs)
             batch_outs = f(ins_batch)
             batch_outs = to_list(batch_outs)
             if batch_index == 0:
@@ -301,12 +328,20 @@ def predict_loop(model, f, ins, batch_size=32, verbose=0, steps=None):
                     outs.append(np.zeros(shape, dtype=batch_out.dtype))
             for i, batch_out in enumerate(batch_outs):
                 outs[i][batch_start:batch_end] = batch_out
+
+            batch_logs['outputs'] = batch_outs
+            callbacks._call_batch_hook('predict', 'end', batch_index, batch_logs)
             if verbose == 1:
                 progbar.update(batch_end)
+        callbacks._call_end_hook('predict')
         return unpack_singleton(outs)
 
 
-def test_loop(model, f, ins, batch_size=None, verbose=0, steps=None):
+def test_loop(model, f, ins,
+              batch_size=None,
+              verbose=0,
+              steps=None,
+              callbacks=None):
     """Abstract method to loop over some data in batches.
 
     # Arguments
@@ -318,6 +353,8 @@ def test_loop(model, f, ins, batch_size=None, verbose=0, steps=None):
         steps: Total number of steps (batches of samples)
             before declaring predictions finished.
             Ignored with the default value of `None`.
+        callbacks: List of callbacks or an instance of
+            `keras.callbacks.CallbackList` to be called during evaluation.
 
     # Returns
         Scalar loss (if the model has a single output and no metrics)
@@ -339,6 +376,24 @@ def test_loop(model, f, ins, batch_size=None, verbose=0, steps=None):
                                     batch_size=batch_size,
                                     steps=steps,
                                     steps_name='steps')
+
+    # Check if callbacks have not been already configured
+    if not isinstance(callbacks, cbks.CallbackList):
+        callbacks = cbks.CallbackList(callbacks)
+        callback_model = model._get_callback_model()
+        callbacks.set_model(callback_model)
+        callback_metrics = []
+        if hasattr(model, 'metrics_names'):
+            callback_metrics = list(model.metrics_names)
+        callback_params = {
+            'batch_size': batch_size,
+            'steps': steps,
+            'samples': num_samples,
+            'verbose': verbose,
+            'metrics': callback_metrics,
+        }
+        callbacks.set_params(callback_params)
+
     outs = []
     if verbose == 1:
         if steps is not None:
@@ -356,8 +411,13 @@ def test_loop(model, f, ins, batch_size=None, verbose=0, steps=None):
         if issparse(ins[i]) and not K.is_sparse(feed[i]):
             indices_for_conversion_to_dense.append(i)
 
+    callbacks.model.stop_training = False
+    callbacks._call_begin_hook('test')
+
     if steps is not None:
         for step in range(steps):
+            batch_logs = {'batch': step, 'size': 1}
+            callbacks._call_batch_hook('test', 'begin', step, batch_logs)
             batch_outs = f(ins)
             if isinstance(batch_outs, list):
                 if step == 0:
@@ -372,6 +432,12 @@ def test_loop(model, f, ins, batch_size=None, verbose=0, steps=None):
                 if step == 0:
                     outs.append(0.)
                 outs[0] += batch_outs
+
+            if hasattr(model, 'metrics_names'):
+                for l, o in zip(model.metrics_names, batch_outs):
+                    batch_logs[l] = o
+            callbacks._call_batch_hook('test', 'end', step, batch_logs)
+
             if verbose == 1:
                 progbar.update(step + 1)
         for i in range(len(outs)):
@@ -390,6 +456,8 @@ def test_loop(model, f, ins, batch_size=None, verbose=0, steps=None):
             for i in indices_for_conversion_to_dense:
                 ins_batch[i] = ins_batch[i].toarray()
 
+            batch_logs = {'batch': batch_index, 'size': len(batch_ids)}
+            callbacks._call_batch_hook('test', 'begin', batch_index, batch_logs)
             batch_outs = f(ins_batch)
             if isinstance(batch_outs, list):
                 if batch_index == 0:
@@ -405,9 +473,15 @@ def test_loop(model, f, ins, batch_size=None, verbose=0, steps=None):
                     outs.append(0.)
                 outs[0] += batch_outs * len(batch_ids)
 
+            if hasattr(model, 'metrics_names'):
+                for l, o in zip(model.metrics_names, batch_outs):
+                    batch_logs[l] = o
+            callbacks._call_batch_hook('test', 'end', batch_index, batch_logs)
+
             if verbose == 1:
                 progbar.update(batch_end)
         for i in range(len(outs)):
             if i not in stateful_metric_indices:
                 outs[i] /= num_samples
+    callbacks._call_end_hook('test')
     return unpack_singleton(outs)
diff --git a/keras/engine/training_generator.py b/keras/engine/training_generator.py
index f74eddf8..34a64489 100644
--- a/keras/engine/training_generator.py
+++ b/keras/engine/training_generator.py
@@ -88,10 +88,8 @@ def fit_generator(model,
     callbacks = cbks.CallbackList(_callbacks)
 
     # it's possible to callback a different model than self:
-    if hasattr(model, 'callback_model') and model.callback_model:
-        callback_model = model.callback_model
-    else:
-        callback_model = model
+    callback_model = model._get_callback_model()
+
     callbacks.set_model(callback_model)
     callbacks.set_params({
         'epochs': epochs,
@@ -100,7 +98,7 @@ def fit_generator(model,
         'do_validation': do_validation,
         'metrics': callback_metrics,
     })
-    callbacks.on_train_begin()
+    callbacks._call_begin_hook('train')
 
     enqueuer = None
     val_enqueuer = None
@@ -168,7 +166,7 @@ def fit_generator(model,
             else:
                 output_generator = generator
 
-        callback_model.stop_training = False
+        callbacks.model.stop_training = False
         # Construct epoch logs.
         epoch_logs = {}
         while epoch < epochs:
@@ -196,8 +194,6 @@ def fit_generator(model,
                                      'a tuple `(x, y, sample_weight)` '
                                      'or `(x, y)`. Found: ' +
                                      str(generator_output))
-                # build batch logs
-                batch_logs = {}
                 if x is None or len(x) == 0:
                     # Handle data tensors support when no input given
                     # step-size = 1 for data tensors
@@ -208,8 +204,8 @@ def fit_generator(model,
                     batch_size = list(x.values())[0].shape[0]
                 else:
                     batch_size = x.shape[0]
-                batch_logs['batch'] = batch_index
-                batch_logs['size'] = batch_size
+                # build batch logs
+                batch_logs = {'batch': batch_index, 'size': batch_size}
                 callbacks.on_batch_begin(batch_index, batch_logs)
 
                 outs = model.train_on_batch(x, y,
@@ -220,17 +216,20 @@ def fit_generator(model,
                 for l, o in zip(out_labels, outs):
                     batch_logs[l] = o
 
-                callbacks.on_batch_end(batch_index, batch_logs)
+                callbacks._call_batch_hook('train', 'end', batch_index, batch_logs)
 
                 batch_index += 1
                 steps_done += 1
 
                 # Epoch finished.
                 if steps_done >= steps_per_epoch and do_validation:
+                    # Note that `callbacks` here is an instance of
+                    # `keras.callbacks.CallbackList`
                     if val_gen:
                         val_outs = model.evaluate_generator(
                             val_enqueuer_gen,
                             validation_steps,
+                            callbacks=callbacks,
                             workers=0)
                     else:
                         # No need for try/except because
@@ -239,18 +238,19 @@ def fit_generator(model,
                             val_x, val_y,
                             batch_size=batch_size,
                             sample_weight=val_sample_weights,
+                            callbacks=callbacks,
                             verbose=0)
                     val_outs = to_list(val_outs)
                     # Same labels assumed.
                     for l, o in zip(out_labels, val_outs):
                         epoch_logs['val_' + l] = o
 
-                if callback_model.stop_training:
+                if callbacks.model.stop_training:
                     break
 
             callbacks.on_epoch_end(epoch, epoch_logs)
             epoch += 1
-            if callback_model.stop_training:
+            if callbacks.model.stop_training:
                 break
 
     finally:
@@ -261,12 +261,13 @@ def fit_generator(model,
             if val_enqueuer is not None:
                 val_enqueuer.stop()
 
-    callbacks.on_train_end()
+    callbacks._call_end_hook('train')
     return model.history
 
 
 def evaluate_generator(model, generator,
                        steps=None,
+                       callbacks=None,
                        max_queue_size=10,
                        workers=1,
                        use_multiprocessing=False,
@@ -303,6 +304,24 @@ def evaluate_generator(model, generator,
                              ' `keras.utils.Sequence` class.')
     enqueuer = None
 
+    # Check if callbacks have not been already configured
+    if not isinstance(callbacks, cbks.CallbackList):
+        callbacks = cbks.CallbackList(callbacks)
+        callback_model = model._get_callback_model()
+        callbacks.set_model(callback_model)
+        callback_metrics = []
+        if hasattr(model, 'metrics_names'):
+            callback_metrics = list(model.metrics_names)
+        callback_params = {
+            'steps': steps,
+            'verbose': verbose,
+            'metrics': callback_metrics,
+        }
+        callbacks.set_params(callback_params)
+
+    callbacks.model.stop_training = False
+    callbacks._call_begin_hook('test')
+
     try:
         if workers > 0:
             if use_sequence_api:
@@ -341,9 +360,6 @@ def evaluate_generator(model, generator,
                                  '(x, y, sample_weight) '
                                  'or (x, y). Found: ' +
                                  str(generator_output))
-            outs = model.test_on_batch(x, y, sample_weight=sample_weight)
-            outs = to_list(outs)
-            outs_per_batch.append(outs)
 
             if x is None or len(x) == 0:
                 # Handle data tensors support when no input given
@@ -359,10 +375,24 @@ def evaluate_generator(model, generator,
                 raise ValueError('Received an empty batch. '
                                  'Batches should contain '
                                  'at least one item.')
+
+            batch_logs = {'batch': steps_done, 'size': batch_size}
+            callbacks._call_batch_hook('test', 'begin', steps_done, batch_logs)
+            outs = model.test_on_batch(x, y, sample_weight=sample_weight)
+            outs = to_list(outs)
+            outs_per_batch.append(outs)
+
+            if hasattr(model, 'metrics_names'):
+                for l, o in zip(model.metrics_names, outs):
+                    batch_logs[l] = o
+            callbacks._call_batch_hook('test', 'end', steps_done, batch_logs)
+
             steps_done += 1
             batch_sizes.append(batch_size)
+
             if verbose == 1:
                 progbar.update(steps_done)
+        callbacks._call_end_hook('test')
 
     finally:
         if enqueuer is not None:
@@ -380,6 +410,7 @@ def evaluate_generator(model, generator,
 
 def predict_generator(model, generator,
                       steps=None,
+                      callbacks=None,
                       max_queue_size=10,
                       workers=1,
                       use_multiprocessing=False,
@@ -406,6 +437,20 @@ def predict_generator(model, generator,
                              ' `keras.utils.Sequence` class.')
     enqueuer = None
 
+    # Check if callbacks have not been already configured
+    if not isinstance(callbacks, cbks.CallbackList):
+        callbacks = cbks.CallbackList(callbacks)
+        callback_model = model._get_callback_model()
+        callbacks.set_model(callback_model)
+        callback_params = {
+            'steps': steps,
+            'verbose': verbose,
+        }
+        callbacks.set_params(callback_params)
+
+    callbacks.model.stop_training = False
+    callbacks._call_begin_hook('predict')
+
     try:
         if workers > 0:
             if use_sequence_api:
@@ -446,6 +491,24 @@ def predict_generator(model, generator,
                 # yields inputs (not targets and sample weights).
                 x = generator_output
 
+            if x is None or len(x) == 0:
+                # Handle data tensors support when no input given
+                # step-size = 1 for data tensors
+                batch_size = 1
+            elif isinstance(x, list):
+                batch_size = x[0].shape[0]
+            elif isinstance(x, dict):
+                batch_size = list(x.values())[0].shape[0]
+            else:
+                batch_size = x.shape[0]
+            if batch_size == 0:
+                raise ValueError('Received an empty batch. '
+                                 'Batches should contain '
+                                 'at least one item.')
+
+            batch_logs = {'batch': steps_done, 'size': batch_size}
+            callbacks._call_batch_hook('predict', 'begin', steps_done, batch_logs)
+
             outs = model.predict_on_batch(x)
             outs = to_list(outs)
 
@@ -455,10 +518,14 @@ def predict_generator(model, generator,
 
             for i, out in enumerate(outs):
                 all_outs[i].append(out)
+
+            batch_logs['outputs'] = outs
+            callbacks._call_batch_hook('predict', 'end', steps_done, batch_logs)
+
             steps_done += 1
             if verbose == 1:
                 progbar.update(steps_done)
-
+        callbacks._call_end_hook('predict')
     finally:
         if enqueuer is not None:
             enqueuer.stop()
