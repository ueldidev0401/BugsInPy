diff --git a/scrapy/spiders/sitemap.py b/scrapy/spiders/sitemap.py
index 89d96c33..4742b3e1 100644
--- a/scrapy/spiders/sitemap.py
+++ b/scrapy/spiders/sitemap.py
@@ -1,83 +1,46 @@
-import re
-import logging
-import six
-
-from scrapy.spiders import Spider
-from scrapy.http import Request, XmlResponse
-from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots
-from scrapy.utils.gz import gunzip, is_gzipped
-
-logger = logging.getLogger(__name__)
-
-
-class SitemapSpider(Spider):
-
-    sitemap_urls = ()
-    sitemap_rules = [('', 'parse')]
-    sitemap_follow = ['']
-    sitemap_alternate_links = False
-
-    def __init__(self, *a, **kw):
-        super(SitemapSpider, self).__init__(*a, **kw)
-        self._cbs = []
-        for r, c in self.sitemap_rules:
-            if isinstance(c, six.string_types):
-                c = getattr(self, c)
-            self._cbs.append((regex(r), c))
-        self._follow = [regex(x) for x in self.sitemap_follow]
-
-    def start_requests(self):
-        for url in self.sitemap_urls:
-            yield Request(url, self._parse_sitemap)
-
-    def _parse_sitemap(self, response):
-        if response.url.endswith('/robots.txt'):
-            for url in sitemap_urls_from_robots(response.text):
-                yield Request(url, callback=self._parse_sitemap)
-        else:
-            body = self._get_sitemap_body(response)
-            if body is None:
-                logger.warning("Ignoring invalid sitemap: %(response)s",
-                               {'response': response}, extra={'spider': self})
-                return
-
-            s = Sitemap(body)
-            if s.type == 'sitemapindex':
-                for loc in iterloc(s, self.sitemap_alternate_links):
-                    if any(x.search(loc) for x in self._follow):
-                        yield Request(loc, callback=self._parse_sitemap)
-            elif s.type == 'urlset':
-                for loc in iterloc(s):
-                    for r, c in self._cbs:
-                        if r.search(loc):
-                            yield Request(loc, callback=c)
-                            break
-
-    def _get_sitemap_body(self, response):
-        """Return the sitemap body contained in the given response,
-        or None if the response is not a sitemap.
-        """
-        if isinstance(response, XmlResponse):
-            return response.body
-        elif is_gzipped(response):
-            return gunzip(response.body)
-        elif response.url.endswith('.xml'):
-            return response.body
-        elif response.url.endswith('.xml.gz'):
-            return gunzip(response.body)
-
-
-def regex(x):
-    if isinstance(x, six.string_types):
-        return re.compile(x)
-    return x
-
-
-def iterloc(it, alt=False):
-    for d in it:
-        yield d['loc']
-
-        # Also consider alternate URLs (xhtml:link rel="alternate")
-        if alt and 'alternate' in d:
-            for l in d['alternate']:
-                yield l
+"""
+Module for processing Sitemaps.
+
+Note: The main purpose of this module is to provide support for the
+SitemapSpider, its API is subject to change without notice.
+"""
+
+import lxml.etree
+from six.moves.urllib.parse import urljoin
+
+
+class Sitemap(object):
+    """Class to parse Sitemap (type=urlset) and Sitemap Index
+    (type=sitemapindex) files"""
+
+    def __init__(self, xmltext):
+        xmlp = lxml.etree.XMLParser(recover=True, remove_comments=True, resolve_entities=False)
+        self._root = lxml.etree.fromstring(xmltext, parser=xmlp)
+        rt = self._root.tag
+        self.type = self._root.tag.split('}', 1)[1] if '}' in rt else rt
+
+    def __iter__(self):
+        for elem in self._root.getchildren():
+            d = {}
+            for el in elem.getchildren():
+                tag = el.tag
+                name = tag.split('}', 1)[1] if '}' in tag else tag
+
+                if name == 'link':
+                    if 'href' in el.attrib:
+                        d.setdefault('alternate', []).append(el.get('href'))
+                else:
+                    d[name] = el.text.strip() if el.text else ''
+
+            if 'loc' in d:
+                yield d
+
+
+def sitemap_urls_from_robots(robots_text, base_url=None):
+    """Return an iterator over all sitemap urls contained in the given
+    robots.txt file
+    """
+    for line in robots_text.splitlines():
+        if line.lstrip().lower().startswith('sitemap:'):
+            url = line.split(':', 1)[1].strip()
+            yield urljoin(base_url, url)
diff --git a/tests/test_spider.py b/tests/test_spider.py
index 1d22c121..079734a6 100644
--- a/tests/test_spider.py
+++ b/tests/test_spider.py
@@ -332,13 +332,17 @@ class SitemapSpiderTest(SpiderTest):
         robots = b"""# Sitemap files
 Sitemap: http://example.com/sitemap.xml
 Sitemap: http://example.com/sitemap-product-index.xml
+Sitemap: HTTP://example.com/sitemap-uppercase.xml
+Sitemap: /sitemap-relative-url.xml
 """
 
         r = TextResponse(url="http://www.example.com/robots.txt", body=robots)
         spider = self.spider_class("example.com")
         self.assertEqual([req.url for req in spider._parse_sitemap(r)],
                          ['http://example.com/sitemap.xml',
-                          'http://example.com/sitemap-product-index.xml'])
+                          'http://example.com/sitemap-product-index.xml',
+                          'http://example.com/sitemap-uppercase.xml',
+                          'http://www.example.com/sitemap-relative-url.xml'])
 
 
 class BaseSpiderDeprecationTest(unittest.TestCase):
